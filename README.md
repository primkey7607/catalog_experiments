# Catalog Materialization Experiments

These are the experiments we carried out whose results are described in Section V-B of our paper here: https://arxiv.org/abs/2103.07532 (see Table 7 for a summary).
Below, we describe how to synthesize datasets and populate a 5W1H+R Normalized database, and Data Vault database based on schema designs we describe in Section IV of our paper, on both SQLite and Neo4j backends according to our experimental setup in Section V-B.
We also describe how to execute the queries used in Section V-B.

In addition, to more concisely represent the schemas we use based on the schema designs we propose in Section IV, we have included them using the peewee ORM representation in the backends/ directory.

## SQLite Instructions
To generate and populate the SQLite databases and run the queries for both normalized and datavault, we run the scripts in the following order:
### 5W1H+R Normalized Database
1. genToNSQLCSV.py: generates the synthetic dataset and populates the normalized database. There are 5 parameters: number of data assets, number of records per table, maximum number of data assets involved in a relationship (rel_size), number of Actions, number of data Sources where data assets are located, and whether the data asset is unique or not. To replicate our paper's experimental setup, use parameters (10000000,10000000,5,10000,10000, False).
2. nsql_queries.py: executes the experiment queries on the normalized database. To run the full set of queries, uncomment the run_tests.execute_full() command. To run a query separately, uncomment the run_tests.execute_qn() command, where n is the number of the query to run.
### 5W1H+R Data Vault Database
3. genToDSQLRecords.py: loads the synthetic dataset generated by genToNSQLCSV.py into the datavault database.
4. dsql_queries.py: executes the experiment queries on the datavault database. To run the full set of queries, uncomment the run_tests.execute_full() command. To run a query separately, uncomment the run_tests.execute_qn() command, where n is the number of the query to run.

Note that to run these scripts, you will need SQLite 3.33.0 or higher for the both data generation and query scripts, and the peewee ORM (version 3.13.3 or higher) for the data generation scripts.

## Neo4j Instructions
To generate and populate the neo4j databases, we need two datasets:
1. The normalized dataset which we generated for SQLite.
2. The datavault dataset, which will be created from the normalized dataset.

### 5W1H+R Normalized Database
Bulk import the normalized dataset using the norm_importCreator.py file:
1. Create a folder called 'baddates/' and move all csv files from the normalized dataset into this folder.
2. In norm_importCreator.py, make sure that all commands except the last two are uncommented.
3. Run norm_importCreator.py from the catalog_experiments/ directory (or whichever directory contains the baddates/ directory)
4. Once norm_importCreator.py is finished running, it should generate:

(a) A new set of csv files, containing both header files, and a clean version of the original data.
(b) a .sh file called 'full_norm_loader.sh'.

These files will be in the directory containing the baddates/ directory.

5. Move this new set of csv files into the neo4j-.../import/ directory, and move the .sh file into the neo4j-.../ directory.
6. Make sure that the --database option in the .sh file matches the name of the database you want to load the files into.
7. Make this full_norm_loader.sh file executable (e.g. "chmod +x") and run it.

This bulk loads the normalized data. To run the queries, go to the execute_full function in nneo4j_queries.py and make sure that all methods of the form self.execute_q*() are uncommented. Then, run nneo4j_queries.py. (Of course, you might want to test them one-by-one, in which case simply run the execute_qn() method to run query n).

### 5W1H+R Data Vault Database
Bulk import the Data Vault dataset by:
1. Running the dv_importCreator.py in the same directory where the normalized csv files are located. As with the normalized bulk import creator, this will generate:

(a) A new set of csv files, containing both new header files for Data Vault files, and a clean version of the original data.
(b) a .sh file called 'full_dv_loader.sh'. 

2. Move this new set of csv files into the neo4j-.../import/ directory, and move the .sh file into the neo4j-.../ directory.
3. Make sure that the --database option in the .sh file matches the name of the database you want to load the files into.
4. Make this .sh file executable (e.g. "chmod +x") and run it.

This bulk loads the data vault data. To run the queries, go to the execute_full function in dneo4j_queries.py and make sure that all methods of the form self.execute_q*() are uncommented. Then, run dneo4j_queries.py. (Of course, you might want to test them one-by-one, in which case simply run the execute_qn() method to run query n).
